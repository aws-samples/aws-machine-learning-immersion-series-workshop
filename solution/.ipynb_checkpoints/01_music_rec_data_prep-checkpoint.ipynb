{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommender Lab 1 : Data Preparation and Process\n",
    "\n",
    "### Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook!\n",
    "\n",
    "* Please make sure to run the 00_overview_arch_data.ipynb in the notebook folder before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "prefix = 'music-recommendation-workshop'\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'new_data_paths' not in locals():\n",
    "    \n",
    "    new_data_paths = [f's3://{bucket}/{prefix}/input/tracks.csv',\n",
    "                     f's3://{bucket}/{prefix}/input/ratings.csv']\n",
    "    %store new_data_paths\n",
    "    \n",
    "else:\n",
    "    print(f'input source is available: {new_data_paths}')\n",
    "\n",
    "%store new_data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering with Amazon SageMaker Processing\n",
    "\n",
    "Amazon SageMaker Processing allows you to run steps for data pre- or post-processing, feature engineering, data validation, or model evaluation workloads on Amazon SageMaker. Processing jobs accept data from Amazon S3 as input and store data into Amazon S3 as output.\n",
    "\n",
    "![processing](https://sagemaker.readthedocs.io/en/stable/_images/amazon_sagemaker_processing_image1.png)\n",
    "\n",
    "Here, we'll import the dataset and transform it with SageMaker Processing, which can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server. In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances.  SageMaker Processing includes off-the-shelf support for Scikit-learn, as well as a Bring Your Own Container option, so it can be used with many different data transformation technologies and tasks.    \n",
    "\n",
    "To use SageMaker Processing, simply supply a Python data preprocessing script as shown below.  For this example, we're using a SageMaker prebuilt Scikit-learn container, which includes many common functions for processing data.  There are few limitations on what kinds of code and operations you can run, and only a minimal contract:  input and output data must be placed in specified directories.  If this is done, SageMaker Processing automatically loads the input data from S3 and uploads transformed data back to S3 when the job is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sklearn SageMaker Processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./code/create_datasets.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "# Parse argument variables passed via the CreateDataset processing step\n",
    "def parse_args() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_dir', type=str, default=\"/opt/ml/processing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def enrich_data(df_tracks: pd.DataFrame, df_ratings: pd.DataFrame):\n",
    "    # Perform one-hot encoding\n",
    "    tracks_tmp = pd.get_dummies(df_tracks, columns=[\"genre\"], prefix=\"genre\")\n",
    "\n",
    "    # calculate danceability\n",
    "    tracks_tmp[\"danceability\"] = 0.3*tracks_tmp.valence + 0.1*tracks_tmp.liveness + 0.1*tracks_tmp.energy\n",
    "\n",
    "    # Join the two dataframes\n",
    "    tracks_rating = pd.merge(tracks_tmp, df_ratings, how='inner', on='trackId')\n",
    "    \n",
    "    num_feat_cols = ['userId', 'energy', 'acousticness', 'valence', 'speechiness', 'instrumentalness', 'liveness', 'tempo', 'danceability', 'genre_Latin', 'genre_Folk',  'genre_Blues', 'genre_Rap', 'genre_Reggae', 'genre_Jazz', 'genre_RnB', 'genre_Country', 'genre_Electronic', 'genre_Pop_Rock']\n",
    "    \n",
    "    df_tmp = tracks_rating[tracks_rating.Rating==5][num_feat_cols]\n",
    "    fivestar_ratings = df_tmp.groupby('userId').mean().add_suffix('_5star').reset_index()\n",
    "\n",
    "    # Drop columns\n",
    "    col_drop = [\"ratingEventId\", \"ts\", \"sessionId\", \"itemInSession\", \"trackId\"]\n",
    "    tracks_rating = tracks_rating.drop(col_drop, axis=1)\n",
    "    \n",
    "    # join five start rating df with tracks_rating dataframe\n",
    "    df_output = pd.merge(tracks_rating, fivestar_ratings, how='inner', on='userId').drop(['userId'], axis=1)\n",
    "    first_col = df_output.pop('Rating')\n",
    "    df_output.insert(0, 'Rating', first_col)\n",
    "    df_output.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "def load_data(file_list: list):\n",
    "    # Define columns to use\n",
    "    use_cols = []\n",
    "    # Concat input files\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        if len(use_cols)==0:\n",
    "            dfs.append(pd.read_csv(file))\n",
    "        else:\n",
    "            dfs.append(pd.read_csv(file, usecols=use_cols))    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def save_files(base_dir: str, df_processed: pd.DataFrame):\n",
    "    \n",
    "    # split data \n",
    "    train, val = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "    val, test = train_test_split(val, test_size=0.05, random_state=42)\n",
    "    logger.info(\"Training dataset shape: {}\\nValidation dataset shape: {}\\nTest dataset shape: {}\\n\".format(train.shape, val.shape, test.shape))\n",
    "\n",
    "    # Write train, test splits to output path\n",
    "    train_output_path = pathlib.Path(f'{base_dir}/output/train')\n",
    "    val_output_path = pathlib.Path(f'{base_dir}/output/val')\n",
    "    test_output_path = pathlib.Path(f'{base_dir}/output/test')\n",
    "    train.to_csv(train_output_path / 'train.csv', header=False, index=False)\n",
    "    val.to_csv(val_output_path / 'validation.csv', header=False, index=False)\n",
    "    test.to_csv(test_output_path / 'test.csv', header=False, index=False)\n",
    "\n",
    "    logger.info('Training, validation, and Testing Sets Created')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def main(base_dir: str, args: argparse.Namespace):\n",
    "    # Input tracks files\n",
    "    input_dir = os.path.join(base_dir, \"input/tracks\")\n",
    "    track_file_list = glob.glob(f\"{input_dir}/*.csv\")\n",
    "    logger.info(f\"Input file list: {track_file_list}\")\n",
    "        \n",
    "    if len(track_file_list) == 0:\n",
    "        raise Exception(f\"No input files found in {input_dir}\")\n",
    "\n",
    "    # Input ratings file\n",
    "    ratings_dir = os.path.join(base_dir, \"input/ratings\")\n",
    "    ratings_file_list = glob.glob(f\"{ratings_dir}/*.csv\")\n",
    "    logger.info(f\"Input file list: {ratings_file_list}\")\n",
    "    if not os.path.exists(ratings_dir):\n",
    "        raise Exception(f\"ratings file does not exist\")\n",
    "\n",
    "    # load data into dataframes\n",
    "    df_tracks = load_data(track_file_list)\n",
    "    df_ratings = load_data(ratings_file_list)\n",
    "    \n",
    "    # Extract and load taxi zones geopandas dataframe\n",
    "    df_processed = enrich_data(df_tracks, df_ratings)\n",
    "    \n",
    "    return save_files(base_dir, df_processed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting preprocessing.\")\n",
    "    args = parse_args()\n",
    "    base_dir = args.base_dir\n",
    "    main(base_dir, args)\n",
    "    logger.info(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_script = os.getcwd() + '/code/create_datasets.py'\n",
    "%store process_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the SageMaker Processing job, we instantiate a `SKLearnProcessor` object.  This object allows you to specify the instance type to use in the job, as well as how many instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_cls = SKLearn\n",
    "framework_version_str=\"0.20.0\"\n",
    "\n",
    "base_job_name = 'sm-music-processing'\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls=est_cls,\n",
    "    framework_version=framework_version_str,\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1, \n",
    "    base_job_name=base_job_name,\n",
    ")\n",
    "processing_job_name = name_from_base(base_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/{prefix}/train/{processing_job_name}\"\n",
    "val_path = f\"s3://{bucket}/{prefix}/val/{processing_job_name}\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test/{processing_job_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(\n",
    "    code='create_datasets.py',\n",
    "    source_dir='code',\n",
    "    arguments = [\n",
    "                 '--base_dir', '/opt/ml/processing',\n",
    "                ],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=[v for v in new_data_paths if 'tracks' in v][0],\n",
    "            destination=\"/opt/ml/processing/input/tracks\",\n",
    "            s3_data_distribution_type=\"FullyReplicated\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=[v for v in new_data_paths if 'ratings' in v][0],\n",
    "            destination=\"/opt/ml/processing/input/ratings\",\n",
    "            s3_data_distribution_type=\"FullyReplicated\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\", destination=train_path),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/output/val\", destination=val_path),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "    ],\n",
    "    job_name=processing_job_name,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_path\n",
    "%store val_path\n",
    "%store test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
